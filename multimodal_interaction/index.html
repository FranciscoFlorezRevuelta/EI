
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../CV-AAL-lab/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Multimodal interaction, datasets, and synthetic generation - Entornos Inteligentes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2afb09e1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#multimodal-interaction-datasets-and-synthetic-generation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Entornos Inteligentes" class="md-header__button md-logo" aria-label="Entornos Inteligentes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Entornos Inteligentes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multimodal interaction, datasets, and synthetic generation
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Entornos Inteligentes" class="md-nav__button md-logo" aria-label="Entornos Inteligentes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Entornos Inteligentes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Summary
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../AmI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction to intelligent environments
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../IoT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Internet of Things
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Computer vision for active assisted living
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Computer vision for active assisted living
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CV_AAL/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Theory
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CV-AAL-lab/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lab
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Multimodal interaction, datasets, and synthetic generation
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Multimodal interaction, datasets, and synthetic generation
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-challenges-in-multimodal-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      Core Challenges in Multimodal Interaction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Challenges in Multimodal Interaction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alignment" class="md-nav__link">
    <span class="md-ellipsis">
      Alignment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    <span class="md-ellipsis">
      Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transference" class="md-nav__link">
    <span class="md-ellipsis">
      Transference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantification" class="md-nav__link">
    <span class="md-ellipsis">
      Quantification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-world-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Real-World Challenges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examples-and-applications-of-multimodality" class="md-nav__link">
    <span class="md-ellipsis">
      Examples and Applications of Multimodality
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Examples and Applications of Multimodality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multimodal-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal architectures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal datasets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moodle-test" class="md-nav__link">
    <span class="md-ellipsis">
      Moodle test
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-challenges-in-multimodal-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      Core Challenges in Multimodal Interaction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Challenges in Multimodal Interaction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alignment" class="md-nav__link">
    <span class="md-ellipsis">
      Alignment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    <span class="md-ellipsis">
      Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transference" class="md-nav__link">
    <span class="md-ellipsis">
      Transference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantification" class="md-nav__link">
    <span class="md-ellipsis">
      Quantification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-world-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Real-World Challenges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examples-and-applications-of-multimodality" class="md-nav__link">
    <span class="md-ellipsis">
      Examples and Applications of Multimodality
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Examples and Applications of Multimodality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multimodal-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal architectures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal datasets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moodle-test" class="md-nav__link">
    <span class="md-ellipsis">
      Moodle test
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="multimodal-interaction-datasets-and-synthetic-generation">Multimodal interaction, datasets, and synthetic generation<a class="headerlink" href="#multimodal-interaction-datasets-and-synthetic-generation" title="Permanent link">&para;</a></h1>
<p><strong>Manuel Benavent Lledó, David Ortiz Pérez, and Jose García Rodríguez</strong></p>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Definition</strong></p>
<p>Modality refers to a certain type of information and/or representation format in which information is stored. There is a wide range of modalities: some raw modalities closer to sensor data such as speech signals and images, that can be expressed as language or detected objects, or going one step further, sentiment intensity or object categories.</p>
</blockquote>
<p><strong>Why do we need to explore multimodal data?</strong></p>
<ul>
<li>From human perspective, we naturally interact with the world through multiple modalities, such as vision, language, and touch. Therefore, building systems that can understand and generate multimodal data enables more natural and intuitive human-computer interaction.</li>
<li>From AI perspective, the information is present in different modalities and therefore it will often show diverse qualities, structures and representations. A comprehensive understanding of the different modalities will provide complementary information about a phenomenon. For example, in understanding a scene, combining visual data with textual descriptions can provide a more comprehensive understanding than using either modality alone.</li>
<li>As a result, leveraging multiple modalities often leads to improved performance in various domains due to the richer representations of the data that enhance the robustness to noise and variability of the models. In this way, the lack of data from a modality (let's say an occlusion in a video) may be compensated with a different one (e.g., audio recordings). Moreover, multimodal data facilitates cross-modal learning, where knowledge from one modality can be transferred to another, enabling better generalization.</li>
</ul>
<h2 id="core-challenges-in-multimodal-interaction">Core Challenges in Multimodal Interaction<a class="headerlink" href="#core-challenges-in-multimodal-interaction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Multimodal systems present a series of technical challenges that are understudied in conventional machine learning. This section covers the basic concepts, you may check bibliography for additional information in this area.</p>
<p>(Optional) <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/"><strong>MultiModal Machine Learning Course from Carnegie Mellon University</strong></a></p>
</blockquote>
<h3 id="representation">Representation<a class="headerlink" href="#representation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Learning representations that reflect cross-modal interactions between individual elements, across different modalities.
  This is a core building block for most multimodal modeling problems. Individually, each modality can be seen as a “local” representation or a representation using holisitic features.</li>
<li><strong>Sub-challenges</strong></li>
</ul>
<p><img alt="Representation. Source CMU." src="../images/representation.png" /></p>
<h3 id="alignment">Alignment<a class="headerlink" href="#alignment" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Identifying and modeling cross-modal connections between all elements of multiple modalities, building from the data structure.
  Most modalities have internal structure with multiple elements:</li>
</ul>
<p><img alt="Alignment 1. Source CMU." src="../images/alignment1.png" />
- <strong>Sub-challenges:</strong></p>
<p><img alt="Alignment 2. Source CMU." src="../images/alignment2.png" /></p>
<h3 id="reasoning">Reasoning<a class="headerlink" href="#reasoning" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structure.
  For example, LLMs are a great source of external knowledge for multimodal reasoning.</li>
<li><strong>Sub-challenges:</strong></li>
</ul>
<p><img alt="Reasoning. Source CMU." src="../images/reasoning.png" /></p>
<h3 id="generation">Generation<a class="headerlink" href="#generation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Learning a generative process to produce raw modalities that reflects cross-modal interactions, structure and coherence.</li>
<li><strong>Sub-challenges:</strong></li>
</ul>
<p><img alt="Generation. Source CMU." src="../images/generation.png" /></p>
<h3 id="transference">Transference<a class="headerlink" href="#transference" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Transfer knowledge between modalities, usually to help the target modality which may be noisy or with limited resources.</li>
</ul>
<p><img alt="Transference 1. Source CMU." src="../images/transference1.png" />
- <strong>Sub-challenges:</strong></p>
<p><img alt="Transference 2. Source CMU." src="../images/transference2.png" /></p>
<h3 id="quantification">Quantification<a class="headerlink" href="#quantification" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition:</strong> Empirical and theoretical study to better understand heterogeneity, cross-modal interactions and the multimodal learning process.</li>
<li><strong>Sub-challenges:</strong></li>
</ul>
<p><img alt="Quantification. Source CMU." src="../images/quantification.png" /></p>
<h3 id="real-world-challenges">Real-World Challenges<a class="headerlink" href="#real-world-challenges" title="Permanent link">&para;</a></h3>
<p>Besides the technical challenges, in the real world multimodal systems have to face additional challenges.</p>
<ul>
<li><strong>Robustness</strong>: to overcome noisy or missing data in some modalities.</li>
<li><strong>Trustworthy</strong>: people who use the system must trust it. It is preferable to have a system that does not provide an output (abstains) rather than making unclear predictions on noisy data or missing data.</li>
<li><strong>Variability</strong>: despite a dataset may be captured with high variability of subjects and locations, there are always unseen events that must be handled.</li>
<li><strong>Fairness</strong>: biases must be quantified and addressed to make the systems fair.</li>
<li><strong>Privacy:</strong> it is a crucial aspect when capturing data.</li>
</ul>
<h2 id="examples-and-applications-of-multimodality">Examples and Applications of Multimodality<a class="headerlink" href="#examples-and-applications-of-multimodality" title="Permanent link">&para;</a></h2>
<h3 id="multimodal-architectures">Multimodal architectures<a class="headerlink" href="#multimodal-architectures" title="Permanent link">&para;</a></h3>
<h4 id="how-to-fuse-data-from-different-modalities"><strong>How to fuse data from different modalities?</strong><a class="headerlink" href="#how-to-fuse-data-from-different-modalities" title="Permanent link">&para;</a></h4>
<blockquote>
<p>(Optional) <strong>Multimodal Learning with Transformers: A Survey.</strong> Peng Xu, Xiatian Zhu, David A. Clifton https://doi.org/10.48550/arXiv.2206.06488</p>
</blockquote>
<p>There are many ways of combining modalities but we can mainly categorize them in early, late or intermediate fusion strategies.</p>
<ul>
<li>Early fusion relies on the combination of modalities since the beginning, after the combination, the data is processed by the model. This method enables learning modality dependencies since the beginning. (See a and b in the figure.)</li>
<li>Late fusion relies on the combination of modalities after being processed by individual models. With this processing of the data, we obtain modality features, which are combined for a multimodal result. This method enables the process of each modality independently, as each modality has different challenges. (See c in the figure.)</li>
<li>Intermediate fusion relies in the combination of modalities while being processed by the models. This method enables learning dependencies since the beginning as well as facing modality challenges kind of individually. Examples are cross-attention strategies. (See e and f in the figure.)</li>
</ul>
<p>In order to make the combinations of data modalities several options are available for combination, including concatenating the data, computing a sum, generating a weighted sum using static weights, weighting by a model parameter that may vary, or computing a product.</p>
<p><img alt="" src="../images/example1.png" /></p>
<h4 id="clip">CLIP<a class="headerlink" href="#clip" title="Permanent link">&para;</a></h4>
<blockquote>
<p>(Optional) <strong>Learning Transferable Visual Models From Natural Language Supervision.</strong> Alec Radford, Jong Wook Kim et al. https://doi.org/10.48550/arXiv.2103.00020</p>
</blockquote>
<p>Use of contrastive learning to align visual and textual modalities. The primary advantage of this model is that it provides a pre-trained representation of the data using both images and text. This pre-existing knowledge is particularly useful in zero-shot learning scenarios for downstream tasks.</p>
<p><img alt="" src="../images/clip1.svg" /></p>
<p><img alt="" src="../images/clip2.svg" /></p>
<h4 id="nerf-and-gaussian-splatting">NeRF and Gaussian Splatting<a class="headerlink" href="#nerf-and-gaussian-splatting" title="Permanent link">&para;</a></h4>
<blockquote>
<p>(Optional) <strong>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.</strong> Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. https://doi.org/10.48550/arXiv.2003.08934</p>
<p>(Optional) <strong>3D Gaussian Splatting for Real-Time Radiance Field Rendering.</strong> Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G. https://doi.org/10.48550/arXiv.2308.04079</p>
</blockquote>
<p>Neural Radiance Fields (NeRF) and Gaussian Splatting stand out as two prominent techniques in 3D scene reconstruction. NeRF captures radiance fields from multiple viewpoints to produce photorealistic scenes. Although proficient at generating high-quality images, NeRF's computational demands render it less ideal for real-time applications, such as those needed for game engines like Unreal Engine, commonly used for synthetic data generation.</p>
<p><img alt="" src="../images/nerf.png" /></p>
<p>The Gaussial Splatting model initiates with camera calibration using Structure-from-Motion (SfM) alongside a sparse point cloud.
At rendering time, a process called Gaussian rasterization transforms each Gaussian particle into the appropriate red, blue and green colored pixels that make up each view.</p>
<p><img alt="" src="../images/gaussian.png" /></p>
<p>This approach yields remarkable results with minimal input. Unlike methods that necessitate Multi-View Stereo (MVS) data, Gaussian Splatting employs an optimization procedure to generate a concise and accurate representation of the scene. Gaussian Splatting excels in producing smooth and continuous visualizations, making it suitable for applications where aesthetics and clarity are crucial. The training time for Gaussian splatting is approximately 50 times faster than NeRFs for the same or better quality.</p>
<p>Groundtruth vs Gaussian Splatting:</p>
<p><img alt="" src="../images/gaussian.gif" /></p>
<h4 id="dalle-2">DALLE-2<a class="headerlink" href="#dalle-2" title="Permanent link">&para;</a></h4>
<blockquote>
<p>(Optional) <strong>Hierarchical Text-Conditional Image Generation with CLIP Latents.</strong> Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen https://doi.org/10.48550/arXiv.2204.06125</p>
</blockquote>
<p>One of the state of the art models in text to image generation. This model uses a pretrained CLIP, we learn a joint representation space for text and images. A CLIP text embedding is fed into the diffusion prior model to generate an image embedding, and the using a diffusion decoder obtaining the final image. Explicitly generating the image embeddings using the diffusion prior improves the diversity of the data, but keeps the semantics and style, with a minimal loss.</p>
<p><img alt="" src="../images/dalle.png" /></p>
<blockquote>
<p>(Optional) <strong>Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation</strong> Dunlap, L., Umino, A., Zhang, H., Yang, J., Gonzalez, J. E., &amp; Darrell, T. https://doi.org/10.48550/arXiv.2305.16289</p>
<p>Link to an example of data augmentation using diffusion models. https://colab.research.google.com/drive/1wRW7yDLsCYC6JjcAzCFgOaHHJcLhCiEo?usp=sharing</p>
</blockquote>
<h4 id="whisper">Whisper<a class="headerlink" href="#whisper" title="Permanent link">&para;</a></h4>
<blockquote>
<p>(Optional) <strong>Robust Speech Recognition via Large-Scale Weak Supervision.</strong> Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever https://doi.org/10.48550/arXiv.2212.04356</p>
</blockquote>
<p>Whisper is the state of the art model in transcribing audio, obtaining the text that has been said on the audio. This model obtains a log mel-spectogram from the audio and uses convolutional neural networks to obtain the audio representation to feed a transformer. With this audio representation, the transformer decodes the text, and adds timestamps depending on which part of the audio is being processed. Whisper outperforms the rest of models, obtaining similar results as professional transcribers.</p>
<p><img alt="" src="../images/whisper1.svg" /></p>
<p><img alt="" src="../images/whisper2.svg" /></p>
<h3 id="multimodal-datasets">Multimodal datasets<a class="headerlink" href="#multimodal-datasets" title="Permanent link">&para;</a></h3>
<p>Data is crucial for deep learning and therefore multimodal datasets have been developed for different purposes. Some examples:</p>
<blockquote>
<p>(Optional) <strong>ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment.</strong> Joseph DelPreto et al. https://action-sense.csail.mit.edu/</p>
<p>(Optional) <strong>Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.</strong>  Amir Zadeh et al. https://doi.org/10.18653/v1/P18-1208</p>
<p>(Optional) <strong>IEMOCAP: interactive emotional dyadic motion capture database.</strong> Carlos Busso et al. https://doi.org/10.1007/s10579-008-9076-6</p>
<p>(Optional) <strong>Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence</strong> Amir Zadeh et al. https://doi.org/10.1109/CVPR.2019.00901</p>
</blockquote>
<ul>
<li><strong>Overview of ActionSense sensors:</strong></li>
</ul>
<p><img alt="" src="../images/dataset.jpg" /></p>
<h3 id="references">References:<a class="headerlink" href="#references" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>Warning</strong></p>
<p>References have been <strong><em>included along the text</em></strong>, please review the material and find tip boxes marked as <strong>Important</strong>, which contain mandatory reading material.</p>
<p>As explained in the previous chapter, <strong>a test will be conducted</strong> including topics covered in this material.</p>
</blockquote>
<h2 id="moodle-test">Moodle test<a class="headerlink" href="#moodle-test" title="Permanent link">&para;</a></h2>
<ul>
<li>The moodle test will be developed during practice session on <strong>Wednesday 7 May at 6pm CET</strong>.</li>
<li>The test has a maximum duration of 30 minutes from the start.</li>
<li>The test consists of 20 triple choice questions.</li>
<li>Each wrong answer subtracts 1/3 of the value of a correct answer.</li>
<li>The mark for the test will be considered as one of the marks for the theoretical part of the course. See the overall evaluation of the course in the general conditions.</li>
<li>The questions will be based on this webpage and all the mandatory readings proposed.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["content.code.copy", "content.tooltips", "content.footnote.tooltips"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>